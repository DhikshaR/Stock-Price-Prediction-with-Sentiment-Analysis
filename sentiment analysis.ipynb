{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Prediction with Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data:\n",
    "\n",
    "This dataset was taken from https://www.kaggle.com/aaron7sun/stocknews/data. This dataset was specifically to created to predict the stockmarket based on the news headlines of over 8 years. The author of the data set has mentioned that train data must be those of before 1/1/2015 and test after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Combined_News_DJIA.csv')\n",
    "train = data[data['Date'] < '2015-01-01']\n",
    "test = data[data['Date'] > '2014-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text found in the dataset is in the form of sentences and these sentences must be broken down into tokens. Tokenisation is a process in which the sentences are broken down into a list of words, almost like a .split() function. This can be done by the inbuilt function like CountVectorizer or TfidfVectorizer from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'commander', 'of', 'Navy', 'air', 'reconnaissance', 'squadron', 'that', 'provides', 'the', 'President', 'and', 'the', 'defense', 'secretary', 'the', 'airborne', 'ability', 'to', 'command', 'the', 'nation', 'nuclear', 'weapons', 'has', 'been', 'relieved', 'of', 'duty']\n"
     ]
    }
   ],
   "source": [
    "example3 = CountVectorizer().build_tokenizer()(train.iloc[3,10])\n",
    "print(example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'commander', 'of', 'Navy', 'air', 'reconnaissance', 'squadron', 'that', 'provides', 'the', 'President', 'and', 'the', 'defense', 'secretary', 'the', 'airborne', 'ability', 'to', 'command', 'the', 'nation', 'nuclear', 'weapons', 'has', 'been', 'relieved', 'of', 'duty']\n"
     ]
    }
   ],
   "source": [
    "ex = TfidfVectorizer().build_tokenizer()(train.iloc[3,10])\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the results of the two methods, we find that it does not consider just the important words but also considers capital letters, conjunctions words, etc. To overcome this, we can create our own tokenizer if required higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(s):\n",
    "    stopwords = set(w.rstrip() for w in open('stopwords.txt')) #defining a set of stop words\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # tokenizer\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words that probably wont be of any relevance\n",
    "    tokens = [t for t in tokens if t not in stopwords] # removing set of stop words\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['commander', 'navy', 'air', 'reconnaissance', 'squadron', 'provides', 'president', 'defense', 'secretary', 'airborne', 'ability', 'command', 'nation', 'nuclear', 'weapons', 'relieved', 'duty']\n"
     ]
    }
   ],
   "source": [
    "eee = custom_tokenizer(train.iloc[3,10])\n",
    "print(eee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing:\n",
    "\n",
    "Now for the purpose of training and testing our model, we will continue using the predefined vectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = []\n",
    "for row in range(0,len(train.index)):\n",
    "    Xtrain.append(' '.join(str(x) for x in train.iloc[row,2:27]))\n",
    "    \n",
    "Xtest = []\n",
    "for row in range(0,len(test.index)):\n",
    "    Xtest.append(' '.join(str(x) for x in test.iloc[row,2:27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "basictrain = cv.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression = logistic_regression.fit(basictrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "basictest = cv.transform(Xtest)\n",
    "predictions = logistic_regression.predict(basictest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0    1\n",
       "Actual            \n",
       "0          61  125\n",
       "1          92  100"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(test[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
